{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041d94a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0f4af02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/ec2-user/anaconda3/envs/pytorch_p310/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/HF_HOME\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/home/ec2-user/SageMaker/HF_HOME'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "!echo $HF_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed493e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30001, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_ckpt='gogamza/kobart-base-v2'\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt) #.to('cuda:0')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "special_tokens_dict = {'additional_special_tokens':['<hl>']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "486c7229",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_PAD_TOKEN_FOR_LOSS = True\n",
    "PADDING=\"max_length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ea11946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset qg_koquad (/home/ec2-user/SageMaker/HF_HOME/datasets/lmqg___qg_koquad/qg_koquad/5.0.1/a6f4a2e67e072b2d20dfa43c30cebc1cb871e8fea6efeeb41e69a630ec59f103)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70bb8638d6474d7fafcf97f0ddc3b4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_korquad = datasets.load_dataset('lmqg/qg_koquad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84be7e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_function(examples):\n",
    "#     inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n",
    "#     targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "#     # inputs = [prefix + inp for inp in inputs]\n",
    "#     model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "#     # Tokenize targets with the `text_target` keyword argument\n",
    "#     labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "#     # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "#     # padding in the loss.\n",
    "#     if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
    "#         labels[\"input_ids\"] = [\n",
    "#             [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "#         ]\n",
    "\n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "#     return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c620888b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ec2-user/SageMaker/HF_HOME/datasets/lmqg___qg_koquad/qg_koquad/5.0.1/a6f4a2e67e072b2d20dfa43c30cebc1cb871e8fea6efeeb41e69a630ec59f103/cache-c1bea79d3ffc8415.arrow\n",
      "Loading cached processed dataset at /home/ec2-user/SageMaker/HF_HOME/datasets/lmqg___qg_koquad/qg_koquad/5.0.1/a6f4a2e67e072b2d20dfa43c30cebc1cb871e8fea6efeeb41e69a630ec59f103/cache-a76bee0018d6541a.arrow\n",
      "Loading cached processed dataset at /home/ec2-user/SageMaker/HF_HOME/datasets/lmqg___qg_koquad/qg_koquad/5.0.1/a6f4a2e67e072b2d20dfa43c30cebc1cb871e8fea6efeeb41e69a630ec59f103/cache-5d8e965861c86160.arrow\n"
     ]
    }
   ],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch[\"paragraph_answer\"], max_length=1024, padding=PADDING, truncation=True)\n",
    "    target_encodings = tokenizer(example_batch[\"question\"], max_length=1024, padding=PADDING, truncation=True)\n",
    "    \n",
    "    if PADDING == \"max_length\" and IGNORE_PAD_TOKEN_FOR_LOSS:\n",
    "        target_encodings[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in target_encodings[\"input_ids\"]\n",
    "        ]\n",
    "    \n",
    "    return {\"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"labels\": target_encodings[\"input_ids\"]}\n",
    "\n",
    "dataset_korquad_pt = dataset_korquad.map(convert_examples_to_features, batched=True)\n",
    "\n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "dataset_korquad_pt.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547071b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c5349a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='QG_kobart_korquad',\n",
    "    num_train_epochs=5,\n",
    "    warmup_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    push_to_hub=False,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    gradient_accumulation_steps=8,\n",
    "    fp16=True,\n",
    "    save_total_limit=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c638b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "    train_dataset=dataset_korquad_pt[\"train\"], \n",
    "    eval_dataset=dataset_korquad_pt[\"validation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef18713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaydrill\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ec2-user/SageMaker/workspace/practice/2306_qg/wandb/run-20230726_094848-zg95x6v5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/daydrill/huggingface/runs/zg95x6v5' target=\"_blank\">misunderstood-haze-31</a></strong> to <a href='https://wandb.ai/daydrill/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/daydrill/huggingface' target=\"_blank\">https://wandb.ai/daydrill/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/daydrill/huggingface/runs/zg95x6v5' target=\"_blank\">https://wandb.ai/daydrill/huggingface/runs/zg95x6v5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='498' max='2130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 498/2130 37:07 < 2:02:08, 0.22 it/s, Epoch 1.17/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.723800</td>\n",
       "      <td>1.684276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.604600</td>\n",
       "      <td>1.584406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fba2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab5b3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc4a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664e9841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e9d835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4753620d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6542a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418872ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26663746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251e252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333af79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee1e511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c723c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.config.use_cache = True  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ecfcbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f579254b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '남부군',\n",
       " 'paragraph_question': \"question: 임창정이 1990년 단역으로 영화배우 첫 데뷔하게 된 영화는?, context: 본관은 풍천이며, 경기도 이천에서 태어났다. 1990년 영화 《남부군》에서 단역으로 영화배우 첫 데뷔에 이어 같은 해 KBS 드라마 《지구인》에서 단역으로 출연하였고 이듬해 MBC 《여명의 눈동자》를 통해 단역으로 출연하였다. 이후 다수의 영화에 단역 및 조역으로 출연하다가 1995년 《거짓같은 진실》을 통해서 가수로 데뷔했으나 노래를 홍보하기 위해 나갔던 라디오에서 이 노래보단, '이미 나에게로'가 좋다고 하고, 본인도 그렇게 느껴 타이틀곡을 <이미 나에게로>로 변경하였다. 1997년 영화 《Beat》의 환규 역으로 출연하였고 그 해 3집 《그때 또 다시》를 발매하여 일약 스타덤에 올랐다. 3집 앨범으로 가요톱10 5주연속 골든컵, 1997년 KBS 가요대상 을 수상하였고 영화 《Beat》로 1997년 대종상영화제 남우조연상과 1998년 백상예술대상 신인연기상을 수상하였다. 1998년 4집 《별이 되어》를 발매하고 활동하며 첫 주연 영화 《엑스트라》가 개봉되었고 이후 꾸준히 앨범과 영화를 병행하며 활동하다 2003년 10집 《Bye》를 마지막으로 영화전념을 위해 가수분야에서는 잠정 은퇴선언하였다.\",\n",
       " 'question': '임창정이 1990년 단역으로 영화배우 첫 데뷔하게 된 영화는?',\n",
       " 'sentence': '1990년 영화 《 남부군 》에서 단역으로 영화배우 첫 데뷔에 이어 같은 해 KBS 드라마 《지구인》에서 단역으로 출연하였고 이듬해 MBC 《여명의 눈동자》를 통해 단역으로 출연하였다.',\n",
       " 'paragraph': \"본관은 풍천이며, 경기도 이천에서 태어났다. 1990년 영화 《남부군》에서 단역으로 영화배우 첫 데뷔에 이어 같은 해 KBS 드라마 《지구인》에서 단역으로 출연하였고 이듬해 MBC 《여명의 눈동자》를 통해 단역으로 출연하였다. 이후 다수의 영화에 단역 및 조역으로 출연하다가 1995년 《거짓같은 진실》을 통해서 가수로 데뷔했으나 노래를 홍보하기 위해 나갔던 라디오에서 이 노래보단, '이미 나에게로'가 좋다고 하고, 본인도 그렇게 느껴 타이틀곡을 <이미 나에게로>로 변경하였다. 1997년 영화 《Beat》의 환규 역으로 출연하였고 그 해 3집 《그때 또 다시》를 발매하여 일약 스타덤에 올랐다. 3집 앨범으로 가요톱10 5주연속 골든컵, 1997년 KBS 가요대상 을 수상하였고 영화 《Beat》로 1997년 대종상영화제 남우조연상과 1998년 백상예술대상 신인연기상을 수상하였다. 1998년 4집 《별이 되어》를 발매하고 활동하며 첫 주연 영화 《엑스트라》가 개봉되었고 이후 꾸준히 앨범과 영화를 병행하며 활동하다 2003년 10집 《Bye》를 마지막으로 영화전념을 위해 가수분야에서는 잠정 은퇴선언하였다.\",\n",
       " 'sentence_answer': '1990년 영화 《 <hl> 남부군 <hl> 》에서 단역으로 영화배우 첫 데뷔에 이어 같은 해 KBS 드라마 《지구인》에서 단역으로 출연하였고 이듬해 MBC 《여명의 눈동자》를 통해 단역으로 출연하였다.',\n",
       " 'paragraph_answer': \"본관은 풍천이며, 경기도 이천에서 태어났다. 1990년 영화 《<hl> 남부군 <hl>》에서 단역으로 영화배우 첫 데뷔에 이어 같은 해 KBS 드라마 《지구인》에서 단역으로 출연하였고 이듬해 MBC 《여명의 눈동자》를 통해 단역으로 출연하였다. 이후 다수의 영화에 단역 및 조역으로 출연하다가 1995년 《거짓같은 진실》을 통해서 가수로 데뷔했으나 노래를 홍보하기 위해 나갔던 라디오에서 이 노래보단, '이미 나에게로'가 좋다고 하고, 본인도 그렇게 느껴 타이틀곡을 <이미 나에게로>로 변경하였다. 1997년 영화 《Beat》의 환규 역으로 출연하였고 그 해 3집 《그때 또 다시》를 발매하여 일약 스타덤에 올랐다. 3집 앨범으로 가요톱10 5주연속 골든컵, 1997년 KBS 가요대상 을 수상하였고 영화 《Beat》로 1997년 대종상영화제 남우조연상과 1998년 백상예술대상 신인연기상을 수상하였다. 1998년 4집 《별이 되어》를 발매하고 활동하며 첫 주연 영화 《엑스트라》가 개봉되었고 이후 꾸준히 앨범과 영화를 병행하며 활동하다 2003년 10집 《Bye》를 마지막으로 영화전념을 위해 가수분야에서는 잠정 은퇴선언하였다.\",\n",
       " 'paragraph_sentence': \"본관은 풍천이며, 경기도 이천에서 태어났다. <hl> 1990년 영화 《 남부군 》에서 단역으로 영화배우 첫 데뷔에 이어 같은 해 KBS 드라마 《지구인》에서 단역으로 출연하였고 이듬해 MBC 《여명의 눈동자》를 통해 단역으로 출연하였다. <hl> 이후 다수의 영화에 단역 및 조역으로 출연하다가 1995년 《거짓같은 진실》을 통해서 가수로 데뷔했으나 노래를 홍보하기 위해 나갔던 라디오에서 이 노래보단, '이미 나에게로'가 좋다고 하고, 본인도 그렇게 느껴 타이틀곡을 <이미 나에게로>로 변경하였다. 1997년 영화 《Beat》의 환규 역으로 출연하였고 그 해 3집 《그때 또 다시》를 발매하여 일약 스타덤에 올랐다. 3집 앨범으로 가요톱10 5주연속 골든컵, 1997년 KBS 가요대상 을 수상하였고 영화 《Beat》로 1997년 대종상영화제 남우조연상과 1998년 백상예술대상 신인연기상을 수상하였다. 1998년 4집 《별이 되어》를 발매하고 활동하며 첫 주연 영화 《엑스트라》가 개봉되었고 이후 꾸준히 앨범과 영화를 병행하며 활동하다 2003년 10집 《Bye》를 마지막으로 영화전념을 위해 가수분야에서는 잠정 은퇴선언하였다.\",\n",
       " 'paragraph_id': '6607610-0-1'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_korquad['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e555e9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"본관은 풍천이며, 경기도 이천에서 태어났다. 1990년 영화 《<hl> 남부군 <hl>》에서 단역으로 영화배우 첫 데뷔에 이어 같은 해 KBS 드라마 《지구인》에서 단역으로 출연하였고 이듬해 MBC 《여명의 눈동자》를 통해 단역으로 출연하였다. 이후 다수의 영화에 단역 및 조역으로 출연하다가 1995년 《거짓같은 진실》을 통해서 가수로 데뷔했으나 노래를 홍보하기 위해 나갔던 라디오에서 이 노래보단, '이미 나에게로'가 좋다고 하고, 본인도 그렇게 느껴 타이틀곡을 <이미 나에게로>로 변경하였다. 1997년 영화 《Beat》의 환규 역으로 출연하였고 그 해 3집 《그때 또 다시》를 발매하여 일약 스타덤에 올랐다. 3집 앨범으로 가요톱10 5주연속 골든컵, 1997년 KBS 가요대상 을 수상하였고 영화 《Beat》로 1997년 대종상영화제 남우조연상과 1998년 백상예술대상 신인연기상을 수상하였다. 1998년 4집 《별이 되어》를 발매하고 활동하며 첫 주연 영화 《엑스트라》가 개봉되었고 이후 꾸준히 앨범과 영화를 병행하며 활동하다 2003년 10집 《Bye》를 마지막으로 영화전념을 위해 가수분야에서는 잠정 은퇴선언하였다.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_korquad['test'][0]['paragraph_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a032ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한반도의 늑대는 남한의 경우 1980년대를 마지막으로 사라졌다. 그리고 동물원 사육개체도 1997년을 마지막으로 죽어버렸다. 늑대가 사라지면서 현재 그 먹이가 되는 고라니나 노루, 멧돼지와 너구리등 일부 야생동물들이 개체수가 너무 늘어나면서 농작물을 먹어 치워 여러가지 문제가 발생하였다. 그러자, 일부 동물 보호단체에서 중국에서 만주산 늑대 4마리를 들여와 재도입계획을 세우고 있다. 또한 민간단체에서도 몽골에서 늑대를 들여와 복원을 추진하고 있으며 <hl> 환경부 <hl>에서는 북한과 연해주에서 늑대를 도입할 계획이다. 하지만 한반도의 늑대 복원계획에 대해 문제가 있다. 대륙에서 들여올 늑대가 한반도 아종이 아니라 다른 아종이기 때문에 외래종의 늑대 도입이 될 가능성이 있다. 현재 늑대가 복원이 되면 증가하여 여러 문제가 있는 멧돼지나 고라니등 번식력이 좋은 우제류등 야생동물 개체수를 조절이 가능하다. 하지만 역시 늑대가 사람을 해칠 가능성 역시 제기되고 있다.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input = dataset_korquad['test'][2]['paragraph_answer']\n",
    "_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9659918",
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = tokenizer(_input, return_tensors='pt', return_token_type_ids=False, max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c5d5317",
   "metadata": {},
   "outputs": [],
   "source": [
    "_input['input_ids'] = _input['input_ids'].to('cuda')\n",
    "_input['attention_mask'] = _input['attention_mask'].to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa9b8bc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gened = model.generate(**_input, \n",
    "    max_new_tokens=100,\n",
    "    early_stopping=True,\n",
    "    do_sample=True,\n",
    "    eos_token_id=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e9ab6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gened_q = tokenizer.decode(gened[0]) #.split('### 답변:')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f49c976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s> 늑대를 복원한 몽골의 연해주에 있는 정부 부처는? 대한민국은? 중국으로? 북한에서 연해주에서 늑대를 도입하고자 계획이 나온 정부 부처는? 어디인가????????? 우리나라 민간단체에서 늑대를 도입하려는 기관은????????????????????????????????????????????</s>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gened_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6ab955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e02a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83a46a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
